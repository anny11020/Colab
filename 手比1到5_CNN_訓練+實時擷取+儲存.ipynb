{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anny11020/Colab/blob/main/%E8%A8%93%E7%B7%B4%E6%89%8B%E5%8B%A212345_CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jl_rejsNpYEd"
      },
      "source": [
        "# 訓練手勢 - CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uCkGf52Wo_Vo",
        "outputId": "6bfb265d-1fb9-48ac-ca30-796478b81466",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import cv2\n",
        "from keras.utils import np_utils\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# 設個picture大小及data路徑\n",
        "pic_size = 128\n",
        "image_path = '/content/drive/My Drive/Colab Notebooks/Gesture/dataset/traindata/'\n",
        "\n",
        "# 印出dataset中各類有幾張image\n",
        "for image_count in os.listdir(image_path):\n",
        "    print(str(len(os.listdir(image_path + image_count))) + \" \" + image_count + \" images\")\n",
        "\n",
        "# 記錄總共有幾張image\n",
        "file_count = 0\n",
        "for floderName in os.listdir(image_path):\n",
        "    for filename in os.listdir(image_path + floderName):\n",
        "        file_count +=1\n",
        "print('all_image_file: ',file_count)\n",
        "\n",
        "# 建立空的np_array (待會填label用)\n",
        "label_default = np.zeros(shape=[file_count])\n",
        "img_default = np.zeros(shape=[file_count,pic_size,pic_size])\n",
        "file_count = 0\n",
        "\n",
        "# 給各個floder中的image上label\n",
        "for floderName in os.listdir(image_path):\n",
        "    for filename in os.listdir(image_path + floderName):\n",
        "\n",
        "        temp = cv2.imread(image_path + floderName + \"/\" + filename,0)\n",
        "        temp = cv2.resize(temp, (pic_size,pic_size))\n",
        "        img_default[file_count] = temp\n",
        "\n",
        "        if floderName == '1':\n",
        "            label_default[file_count] = 0\n",
        "        elif floderName == '2':\n",
        "            label_default[file_count] = 1\n",
        "        elif floderName == '3':\n",
        "            label_default[file_count] = 2\n",
        "        elif floderName == '4':\n",
        "            label_default[file_count] = 3\n",
        "        elif floderName == '5':\n",
        "            label_default[file_count] = 4\n",
        "\n",
        "        file_count +=1\n",
        "\n",
        "# reshape成丟進model input的dimension\n",
        "img_default = img_default.reshape(file_count,pic_size,pic_size,1)\n",
        "img_default.shape\n",
        "\n",
        "label_onehot=np_utils.to_categorical(label_default) # 做onehot encoding\n",
        "print('label_onehot[0]:{},label_dim:{},shape:{}'.format(label_onehot[0],label_onehot.ndim,label_onehot.shape)) # Label(Encoding結果 , 維度, shape)\n",
        "img_default = img_default / 255.0 # 做 normalization\n",
        "\n",
        "\n",
        "random_seed  = 3 # 隨機分割\n",
        "x_train, x_test, y_train, y_test = train_test_split(img_default, label_onehot, test_size = 0.2, random_state=random_seed) # 切分訓練及測試集\n",
        "print('x_train.shape:{}\\n,y_train.shape:{}\\nx_test.shape:{}\\ny_test.shape:{}'.format(x_train.shape, y_train.shape, x_test.shape, y_test.shape)) #(train_img, train_label, test_img, test_label)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5 4 images\n",
            "5 3 images\n",
            "5 2 images\n",
            "5 1 images\n",
            "5 5 images\n",
            "all_image_file:  25\n",
            "label_onehot[0]:[0. 0. 0. 1. 0.],label_dim:2,shape:(25, 5)\n",
            "x_train.shape:(20, 128, 128, 1)\n",
            ",y_train.shape:(20, 5)\n",
            "x_test.shape:(5, 128, 128, 1)\n",
            "y_test.shape:(5, 5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a-m4GQUkpPd6"
      },
      "source": [
        "# 儲存 model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EcNM2ljHpOjj",
        "outputId": "5da4dcd8-da88-42ee-e0dd-c2afb6ea2b10",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# ... preprocessing ...\n",
        "\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "classes = 5 # 有五種分類\n",
        "\n",
        "model = Sequential([\n",
        "    Conv2D(64, 3, activation='relu', input_shape=(pic_size,pic_size,1)),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "    Conv2D(32, 3, activation='relu'),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "\n",
        "    Dense(classes, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer=Adam(lr=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model.summary()\n",
        "model.fit(x_train,y_train,validation_data=(x_test,y_test),epochs=50)\n",
        "\n",
        "# 將訓練好的model儲存成json及h5檔\n",
        "import json\n",
        "model_json = model.to_json()\n",
        "with open(\"/content/drive/My Drive/Colab Notebooks/Gesture/model_trained.json\", \"w\") as json_file:\n",
        "    json.dump(model_json, json_file)\n",
        "model.save(\"/content/drive/My Drive/Colab Notebooks/Gesture/model_trained.h5\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 126, 126, 64)      640       \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 63, 63, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 61, 61, 32)        18464     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 30, 30, 32)        0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 28800)             0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 128)               3686528   \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 5)                 645       \n",
            "=================================================================\n",
            "Total params: 3,706,277\n",
            "Trainable params: 3,706,277\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/50\n",
            "1/1 [==============================] - 0s 217ms/step - loss: 1.5608 - accuracy: 0.2500 - val_loss: 1.5964 - val_accuracy: 0.2000\n",
            "Epoch 2/50\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 1.0044 - accuracy: 0.6000 - val_loss: 1.4140 - val_accuracy: 0.4000\n",
            "Epoch 3/50\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 0.4416 - accuracy: 0.8500 - val_loss: 1.3563 - val_accuracy: 0.4000\n",
            "Epoch 4/50\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 0.1439 - accuracy: 1.0000 - val_loss: 1.4187 - val_accuracy: 0.4000\n",
            "Epoch 5/50\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 0.0639 - accuracy: 0.9500 - val_loss: 1.4516 - val_accuracy: 0.8000\n",
            "Epoch 6/50\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 0.0035 - accuracy: 1.0000 - val_loss: 1.6551 - val_accuracy: 0.8000\n",
            "Epoch 7/50\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 0.0160 - accuracy: 1.0000 - val_loss: 1.9138 - val_accuracy: 0.8000\n",
            "Epoch 8/50\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 0.0418 - accuracy: 1.0000 - val_loss: 2.2246 - val_accuracy: 0.8000\n",
            "Epoch 9/50\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 0.0445 - accuracy: 0.9500 - val_loss: 2.6033 - val_accuracy: 0.8000\n",
            "Epoch 10/50\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 0.0856 - accuracy: 0.9500 - val_loss: 2.8850 - val_accuracy: 0.8000\n",
            "Epoch 11/50\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 0.0409 - accuracy: 1.0000 - val_loss: 3.1759 - val_accuracy: 0.8000\n",
            "Epoch 12/50\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 0.0106 - accuracy: 1.0000 - val_loss: 3.4090 - val_accuracy: 0.8000\n",
            "Epoch 13/50\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 2.0435e-04 - accuracy: 1.0000 - val_loss: 3.6163 - val_accuracy: 0.8000\n",
            "Epoch 14/50\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 3.8116 - val_accuracy: 0.8000\n",
            "Epoch 15/50\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 0.0025 - accuracy: 1.0000 - val_loss: 3.9983 - val_accuracy: 0.8000\n",
            "Epoch 16/50\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 0.1902 - accuracy: 0.9500 - val_loss: 3.9992 - val_accuracy: 0.8000\n",
            "Epoch 17/50\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 0.0641 - accuracy: 0.9500 - val_loss: 3.9534 - val_accuracy: 0.8000\n",
            "Epoch 18/50\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 0.0139 - accuracy: 1.0000 - val_loss: 3.9086 - val_accuracy: 0.8000\n",
            "Epoch 19/50\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 5.5384e-04 - accuracy: 1.0000 - val_loss: 3.8663 - val_accuracy: 0.8000\n",
            "Epoch 20/50\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 4.7325e-06 - accuracy: 1.0000 - val_loss: 3.8295 - val_accuracy: 0.8000\n",
            "Epoch 21/50\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 1.6291e-04 - accuracy: 1.0000 - val_loss: 3.7911 - val_accuracy: 0.8000\n",
            "Epoch 22/50\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 6.3953e-06 - accuracy: 1.0000 - val_loss: 3.7589 - val_accuracy: 0.8000\n",
            "Epoch 23/50\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 0.0046 - accuracy: 1.0000 - val_loss: 3.7331 - val_accuracy: 0.8000\n",
            "Epoch 24/50\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 7.9008e-04 - accuracy: 1.0000 - val_loss: 3.7137 - val_accuracy: 0.8000\n",
            "Epoch 25/50\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 1.2974e-05 - accuracy: 1.0000 - val_loss: 3.6969 - val_accuracy: 0.8000\n",
            "Epoch 26/50\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 1.4841e-06 - accuracy: 1.0000 - val_loss: 3.6830 - val_accuracy: 0.8000\n",
            "Epoch 27/50\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 4.2276e-04 - accuracy: 1.0000 - val_loss: 3.6696 - val_accuracy: 0.8000\n",
            "Epoch 28/50\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 0.0347 - accuracy: 1.0000 - val_loss: 3.6159 - val_accuracy: 0.8000\n",
            "Epoch 29/50\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 1.2474e-05 - accuracy: 1.0000 - val_loss: 3.5681 - val_accuracy: 0.8000\n",
            "Epoch 30/50\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 3.5210 - val_accuracy: 0.8000\n",
            "Epoch 31/50\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 0.0825 - accuracy: 0.9500 - val_loss: 3.3433 - val_accuracy: 0.8000\n",
            "Epoch 32/50\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 6.8906e-04 - accuracy: 1.0000 - val_loss: 3.1881 - val_accuracy: 0.8000\n",
            "Epoch 33/50\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 6.3535e-06 - accuracy: 1.0000 - val_loss: 3.0525 - val_accuracy: 0.8000\n",
            "Epoch 34/50\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 8.6426e-07 - accuracy: 1.0000 - val_loss: 2.9344 - val_accuracy: 0.8000\n",
            "Epoch 35/50\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 3.2361e-05 - accuracy: 1.0000 - val_loss: 2.8325 - val_accuracy: 0.8000\n",
            "Epoch 36/50\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 3.7465e-05 - accuracy: 1.0000 - val_loss: 2.7436 - val_accuracy: 0.8000\n",
            "Epoch 37/50\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 4.7682e-06 - accuracy: 1.0000 - val_loss: 2.6670 - val_accuracy: 0.8000\n",
            "Epoch 38/50\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 8.1062e-07 - accuracy: 1.0000 - val_loss: 2.6000 - val_accuracy: 0.8000\n",
            "Epoch 39/50\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 1.8690e-05 - accuracy: 1.0000 - val_loss: 2.5404 - val_accuracy: 0.8000\n",
            "Epoch 40/50\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 2.5093e-06 - accuracy: 1.0000 - val_loss: 2.4878 - val_accuracy: 0.8000\n",
            "Epoch 41/50\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 1.0272e-04 - accuracy: 1.0000 - val_loss: 2.4425 - val_accuracy: 0.8000\n",
            "Epoch 42/50\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 1.2707e-04 - accuracy: 1.0000 - val_loss: 2.4027 - val_accuracy: 0.8000\n",
            "Epoch 43/50\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 2.3698e-04 - accuracy: 1.0000 - val_loss: 2.3681 - val_accuracy: 0.8000\n",
            "Epoch 44/50\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 7.3739e-05 - accuracy: 1.0000 - val_loss: 2.3376 - val_accuracy: 0.8000\n",
            "Epoch 45/50\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 1.6778e-04 - accuracy: 1.0000 - val_loss: 2.3116 - val_accuracy: 0.8000\n",
            "Epoch 46/50\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 2.2915 - val_accuracy: 0.8000\n",
            "Epoch 47/50\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 1.0948e-05 - accuracy: 1.0000 - val_loss: 2.2735 - val_accuracy: 0.8000\n",
            "Epoch 48/50\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 7.0871e-05 - accuracy: 1.0000 - val_loss: 2.2584 - val_accuracy: 0.8000\n",
            "Epoch 49/50\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 4.7608e-05 - accuracy: 1.0000 - val_loss: 2.2457 - val_accuracy: 0.8000\n",
            "Epoch 50/50\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 2.6570e-04 - accuracy: 1.0000 - val_loss: 2.2354 - val_accuracy: 0.8000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rkYaB8Gz2SUW"
      },
      "source": [
        "# h5 轉成 pb\n",
        "https://stackoverflow.com/questions/60005661/tensorflow-2-0-convert-keras-model-to-pb-file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wE7-hteo2Rps"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "pre_model = tf.keras.models.load_model(\"final_model.h5\")\n",
        "pre_model.save(\"saved_model\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VC__K9HJ2p3l"
      },
      "source": [
        "# pb 轉成 byte\n",
        "https://github.com/llSourcell/Unity_ML_Agents/blob/master/docs/Using-TensorFlow-Sharp-in-Unity-(Experimental).md"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o99IZh3Kpkhx"
      },
      "source": [
        "# 擷取影像 - OpenCV"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oodv9swFpkK5"
      },
      "source": [
        "import cv2\n",
        "cap = cv2.VideoCapture(0)\n",
        "\n",
        "while True:\n",
        "    _, FrameImage = cap.read() # 讀取鏡頭畫面\n",
        "    FrameImage = cv2.flip(FrameImage, 1) # 圖像水平翻轉\n",
        "    cv2.imshow(\"Webcam\", FrameImage) # 顯示鏡頭畫面\n",
        "    interrupt = cv2.waitKey(10)\n",
        "    if interrupt & 0xFF == ord('q'): # 觸及小寫q，關閉webcam\n",
        "      break\n",
        "\n",
        "cap.release()\n",
        "cv2.destroyAllWindows()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iia0LEHLpyk2"
      },
      "source": [
        "# 蒐集 Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MSRvFp7Gpyuk"
      },
      "source": [
        "import cv2\n",
        "\n",
        "# ... get webcam and ROI_preprocessed ...\n",
        "\n",
        "while True:\n",
        "  cv2.imwrite('./test/handdata'+str(i)+'.jpg',output2)\n",
        "  i += 1\n",
        "  cv2.waitKey(100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oFL_7brFp3pM"
      },
      "source": [
        "# 畫面前處理"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MICN0dhGp3O5"
      },
      "source": [
        "import cv2\n",
        "CLIP_X1,CLIP_Y1,CLIP_X2,CLIP_Y2 = 160,140,400,360 # ROI's size\n",
        "i = 0 # 用來記錄之後要新增的image\n",
        "wzs = 158 # 調整二值化的閥值變數\n",
        "image_q = cv2.THRESH_BINARY # 調整二值化的模式\n",
        "\n",
        "# ... Get webcam ...\n",
        "\n",
        "cv2.rectangle(FrameImage, (CLIP_X1, CLIP_Y1), (CLIP_X2, CLIP_Y2), (0,255,0) ,1) # 框出ROI位置\n",
        "ROI = FrameImage[CLIP_Y1:CLIP_Y2, CLIP_X1:CLIP_X2] # ROI的大小\n",
        "ROI = cv2.resize(ROI, (128, 128))  # ROI RESIZE (配合訓練大小)\n",
        "ROI = cv2.cvtColor(ROI, cv2.COLOR_BGR2GRAY) # ROI 轉灰階\n",
        "SHOWROI = cv2.resize(ROI, (256, 256)) # ROI resize\n",
        "# Threshold Binary：即二值化，將大於閾值的灰度值設為最大灰度值，小於閾值的值設為0。\n",
        "_, output = cv2.threshold(ROI, wzs, 255, image_q) # Black Background is better for prediction\n",
        "_, output2 = cv2.threshold(SHOWROI, wzs, 255, image_q) # 用來顯示\n",
        "cv2.imshow(\"ROI\", output2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uRqWpcNdp_0o"
      },
      "source": [
        "# 預測及辨識"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RWiJBs9_p3R8"
      },
      "source": [
        "import json\n",
        "import pygame\n",
        "import sys, os\n",
        "import operator\n",
        "from PIL import Image\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.models import model_from_json\n",
        "\n",
        "# 使用pygame來建一個預測視窗\n",
        "pygame.init()\n",
        "screen = pygame.display.set_mode((400,400),pygame.RESIZABLE)\n",
        "\n",
        "# ... Get webcam ... ( in while True )\n",
        "\n",
        "# loading model\n",
        "with open('model_in_json_CNN_SW_IOU.json','r') as f:\n",
        "    model_json = json.load(f)\n",
        "loaded_model = model_from_json(model_json)\n",
        "loaded_model.load_weights('model_Binary_CNN_SW_IOU.h5')\n",
        "# ... ROI_preprocessed ...\n",
        "result = loaded_model.predict(output2.reshape(1,128, 128, 1)) # 若是訓練彩色，則須把1改成3個channel (1,128,128,3) 配合model輸入的dimension\n",
        "predict =   { '1':    result[0][0], # 看訓練幾類，給予多少的result\n",
        "              '2':    result[0][1],\n",
        "              '3':    result[0][2],\n",
        "              '4':    result[0][3],\n",
        "              '5':    result[0][4],\n",
        "            }\n",
        "# print(predict)  # 有需要可以print出來看一下各類predict的分數\n",
        "predict = sorted(predict.items(), key=operator.itemgetter(1), reverse=True) # 分數較高者會sort至第一位\n",
        "\n",
        "# 這邊是取對應預測的image，沒有則是顯示nosign\n",
        "if(predict[0][1] == 1.0):\n",
        "  predict_img  = pygame.image.load(os.getcwd() + '/dataset/images/' + predict[0][0] + '.jpg')\n",
        "else:\n",
        "  predict_img  = pygame.image.load(os.getcwd() + '/dataset/images/nosign.png')\n",
        "predict_img = pygame.transform.scale(predict_img, (400, 400))\n",
        "screen.blit(predict_img, (0,0))\n",
        "pygame.display.flip()\n",
        "\n",
        "pygame.quit()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AzXHkc8FqI74"
      },
      "source": [
        "# 新增 Command"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-QOk_hTHqLGW"
      },
      "source": [
        "import cv2\n",
        "\n",
        "# ... get webcam and ROI_preprocessed ...\n",
        "\n",
        "while True:\n",
        "\n",
        "  # 在ROI上觸及英數字，來反饋效果\n",
        "\n",
        "  interrupt = cv2.waitKey(10)\n",
        "    if interrupt & 0xFF == ord('l'): # lower wzs quality (小寫'l'改threshold的wzs，將標準調低)\n",
        "      wzs = wzs - 5\n",
        "    elif interrupt & 0xFF == ord('u'): # upper wzs quality (小寫'u'改threshold的wzs，將標準調高)\n",
        "      wzs = wzs + 5\n",
        "    elif interrupt & 0xFF == ord ('s'): # save dataset (小寫's'儲存當下frame)\n",
        "      cv2.imwrite('handdata'+str(random.randint(1,9999))+'.jpg',output2)\n",
        "    elif interrupt & 0xFF == ord ('c'): # change THRESH_BINARY TO THRESH_BINARY_INV (小寫'c'改treshold的image_q，背景黑白對調)\n",
        "      if image_q == cv2.THRESH_BINARY_INV:\n",
        "        image_q = cv2.THRESH_BINARY\n",
        "      else:\n",
        "        image_q = cv2.THRESH_BINARY_INV\n",
        "    if interrupt & 0xFF == ord('q'): # esc key\n",
        "        break"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
