{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anny11020/Colab/blob/main/CheXNet_ChexnetTrainer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2-x1rQp9xmX-"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import time\n",
        "import sys"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0S1IdtjOxn07"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as tfunc\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "import torch.nn.functional as func"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hculZZhhxo4j",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "0623769a-bea3-4b59-a770-5649c5f9b219"
      },
      "source": [
        "from sklearn.metrics.ranking import roc_auc_score"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.metrics.ranking module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.metrics. Anything that cannot be imported from sklearn.metrics is now part of the private API.\n",
            "  warnings.warn(message, FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oVozxwIfTvmp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "342887b8-c409-46b1-ae1f-8beab00ceb69"
      },
      "source": [
        "!ls /content/drive/My\\ Drive/Colab\\ Notebooks/Pytorch/CheXNet"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ChexnetTrainer.ipynb  DensenetModels.ipynb  Main.ipynb\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z_WJimYTUihZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "c3bdbb5e-5833-470c-91ec-accb8538b100"
      },
      "source": [
        "!pip install import-ipynb\n",
        "import import_ipynb\n",
        "# Install the PyDrive wrapper & import libraries.\n",
        "# This only needs to be done once per notebook.\n",
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "# Authenticate and create the PyDrive client.\n",
        "# This only needs to be done once per notebook.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: import-ipynb in /usr/local/lib/python3.6/dist-packages (0.1.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i1d7xOZRWc7a"
      },
      "source": [
        "# Copy the link and remove the front part of the link (i.e. https://drive.google.com/open?id=) to get the file ID.\n",
        "module_DensenetModels = drive.CreateFile({'id':'1DEBiRnkzyiszvCm2m6mu4ZLHxfdDBohT'})\n",
        "module_DensenetModels.GetContentFile('DensenetModels.ipynb')\n",
        "import DensenetModels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NsWeCJ4LWe9v"
      },
      "source": [
        "# Copy the link and remove the front part of the link (i.e. https://drive.google.com/open?id=) to get the file ID.\n",
        "module_DatasetGenerator = drive.CreateFile({'id':'14TWLIDm-r_JgZGYJaghUHtDGLRcUzBvo'})\n",
        "module_DatasetGenerator.GetContentFile('DatasetGenerator.ipynb')\n",
        "import DatasetGenerator"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0R1E67Erxqpk"
      },
      "source": [
        "from DensenetModels import DenseNet121\n",
        "from DensenetModels import DenseNet169\n",
        "from DensenetModels import DenseNet201\n",
        "#from DatasetGenerator import DatasetGenerator"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MwRuwjNVxKia"
      },
      "source": [
        "class ChexnetTrainer ():\n",
        "\n",
        "    #---- Train the densenet network\n",
        "    #---- pathDirData - path to the directory that contains images\n",
        "    #---- pathFileTrain - path to the file that contains image paths and label pairs (training set)\n",
        "    #---- pathFileVal - path to the file that contains image path and label pairs (validation set)\n",
        "    #---- nnArchitecture - model architecture 'DENSE-NET-121', 'DENSE-NET-169' or 'DENSE-NET-201'\n",
        "    #---- nnIsTrained - if True, uses pre-trained version of the network (pre-trained on imagenet)\n",
        "    #---- nnClassCount - number of output classes\n",
        "    #---- trBatchSize - batch size\n",
        "    #---- trMaxEpoch - number of epochs\n",
        "    #---- transResize - size of the image to scale down to (not used in current implementation)\n",
        "    #---- transCrop - size of the cropped image\n",
        "    #---- launchTimestamp - date/time, used to assign unique name for the checkpoint file\n",
        "    #---- checkpoint - if not None loads the model and continues training\n",
        "\n",
        "    def train (pathDirData, pathFileTrain, pathFileVal, nnArchitecture, nnIsTrained, nnClassCount, trBatchSize, trMaxEpoch, transResize, transCrop, launchTimestamp, checkpoint):\n",
        "\n",
        "\n",
        "        #-------------------- SETTINGS: NETWORK ARCHITECTURE\n",
        "        if nnArchitecture == 'DENSE-NET-121': model = DenseNet121(nnClassCount, nnIsTrained).cuda()\n",
        "        elif nnArchitecture == 'DENSE-NET-169': model = DenseNet169(nnClassCount, nnIsTrained).cuda()\n",
        "        elif nnArchitecture == 'DENSE-NET-201': model = DenseNet201(nnClassCount, nnIsTrained).cuda()\n",
        "\n",
        "        model = torch.nn.DataParallel(model).cuda()\n",
        "\n",
        "        #-------------------- SETTINGS: DATA TRANSFORMS\n",
        "        normalize = transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "\n",
        "        transformList = []\n",
        "        transformList.append(transforms.RandomResizedCrop(transCrop))\n",
        "        transformList.append(transforms.RandomHorizontalFlip())\n",
        "        transformList.append(transforms.ToTensor())\n",
        "        transformList.append(normalize)\n",
        "        transformSequence=transforms.Compose(transformList)\n",
        "\n",
        "        #-------------------- SETTINGS: DATASET BUILDERS\n",
        "        datasetTrain = DatasetGenerator(pathImageDirectory=pathDirData, pathDatasetFile=pathFileTrain, transform=transformSequence)\n",
        "        datasetVal =   DatasetGenerator(pathImageDirectory=pathDirData, pathDatasetFile=pathFileVal, transform=transformSequence)\n",
        "\n",
        "        dataLoaderTrain = DataLoader(dataset=datasetTrain, batch_size=trBatchSize, shuffle=True,  num_workers=24, pin_memory=True)\n",
        "        dataLoaderVal = DataLoader(dataset=datasetVal, batch_size=trBatchSize, shuffle=False, num_workers=24, pin_memory=True)\n",
        "\n",
        "        #-------------------- SETTINGS: OPTIMIZER & SCHEDULER\n",
        "        optimizer = optim.Adam (model.parameters(), lr=0.0001, betas=(0.9, 0.999), eps=1e-08, weight_decay=1e-5)\n",
        "        scheduler = ReduceLROnPlateau(optimizer, factor = 0.1, patience = 5, mode = 'min')\n",
        "\n",
        "        #-------------------- SETTINGS: LOSS\n",
        "        loss = torch.nn.BCELoss(size_average = True)\n",
        "\n",
        "        #---- Load checkpoint\n",
        "        if checkpoint != None:\n",
        "            modelCheckpoint = torch.load(checkpoint)\n",
        "            model.load_state_dict(modelCheckpoint['state_dict'])\n",
        "            optimizer.load_state_dict(modelCheckpoint['optimizer'])\n",
        "\n",
        "\n",
        "        #---- TRAIN THE NETWORK\n",
        "\n",
        "        lossMIN = 100000\n",
        "\n",
        "        for epochID in range (0, trMaxEpoch):\n",
        "\n",
        "            timestampTime = time.strftime(\"%H%M%S\")\n",
        "            timestampDate = time.strftime(\"%d%m%Y\")\n",
        "            timestampSTART = timestampDate + '-' + timestampTime\n",
        "\n",
        "            ChexnetTrainer.epochTrain (model, dataLoaderTrain, optimizer, scheduler, trMaxEpoch, nnClassCount, loss)\n",
        "            lossVal, losstensor = ChexnetTrainer.epochVal (model, dataLoaderVal, optimizer, scheduler, trMaxEpoch, nnClassCount, loss)\n",
        "\n",
        "            timestampTime = time.strftime(\"%H%M%S\")\n",
        "            timestampDate = time.strftime(\"%d%m%Y\")\n",
        "            timestampEND = timestampDate + '-' + timestampTime\n",
        "\n",
        "            scheduler.step(losstensor.data[0])\n",
        "\n",
        "            if lossVal < lossMIN:\n",
        "                lossMIN = lossVal\n",
        "                torch.save({'epoch': epochID + 1, 'state_dict': model.state_dict(), 'best_loss': lossMIN, 'optimizer' : optimizer.state_dict()}, 'm-' + launchTimestamp + '.pth.tar')\n",
        "                print ('Epoch [' + str(epochID + 1) + '] [save] [' + timestampEND + '] loss= ' + str(lossVal))\n",
        "            else:\n",
        "                print ('Epoch [' + str(epochID + 1) + '] [----] [' + timestampEND + '] loss= ' + str(lossVal))\n",
        "\n",
        "    #--------------------------------------------------------------------------------\n",
        "\n",
        "    def epochTrain (model, dataLoader, optimizer, scheduler, epochMax, classCount, loss):\n",
        "\n",
        "        model.train()\n",
        "\n",
        "        for batchID, (input, target) in enumerate (dataLoader):\n",
        "\n",
        "            target = target.cuda(async = True)\n",
        "\n",
        "            varInput = torch.autograd.Variable(input)\n",
        "            varTarget = torch.autograd.Variable(target)\n",
        "            varOutput = model(varInput)\n",
        "\n",
        "            lossvalue = loss(varOutput, varTarget)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            lossvalue.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "    #--------------------------------------------------------------------------------\n",
        "\n",
        "    def epochVal (model, dataLoader, optimizer, scheduler, epochMax, classCount, loss):\n",
        "\n",
        "        model.eval ()\n",
        "\n",
        "        lossVal = 0\n",
        "        lossValNorm = 0\n",
        "\n",
        "        losstensorMean = 0\n",
        "\n",
        "        for i, (input, target) in enumerate (dataLoader):\n",
        "\n",
        "            target = target.cuda(async=True)\n",
        "\n",
        "            varInput = torch.autograd.Variable(input, volatile=True)\n",
        "            varTarget = torch.autograd.Variable(target, volatile=True)\n",
        "            varOutput = model(varInput)\n",
        "\n",
        "            losstensor = loss(varOutput, varTarget)\n",
        "            losstensorMean += losstensor\n",
        "\n",
        "            lossVal += losstensor.data[0]\n",
        "            lossValNorm += 1\n",
        "\n",
        "        outLoss = lossVal / lossValNorm\n",
        "        losstensorMean = losstensorMean / lossValNorm\n",
        "\n",
        "        return outLoss, losstensorMean\n",
        "\n",
        "    #--------------------------------------------------------------------------------\n",
        "\n",
        "    #---- Computes area under ROC curve\n",
        "    #---- dataGT - ground truth data\n",
        "    #---- dataPRED - predicted data\n",
        "    #---- classCount - number of classes\n",
        "\n",
        "    def computeAUROC (dataGT, dataPRED, classCount):\n",
        "\n",
        "        outAUROC = []\n",
        "\n",
        "        datanpGT = dataGT.cpu().numpy()\n",
        "        datanpPRED = dataPRED.cpu().numpy()\n",
        "\n",
        "        for i in range(classCount):\n",
        "            outAUROC.append(roc_auc_score(datanpGT[:, i], datanpPRED[:, i]))\n",
        "\n",
        "        return outAUROC\n",
        "\n",
        "\n",
        "    #--------------------------------------------------------------------------------\n",
        "\n",
        "    #---- Test the trained network\n",
        "    #---- pathDirData - path to the directory that contains images\n",
        "    #---- pathFileTrain - path to the file that contains image paths and label pairs (training set)\n",
        "    #---- pathFileVal - path to the file that contains image path and label pairs (validation set)\n",
        "    #---- nnArchitecture - model architecture 'DENSE-NET-121', 'DENSE-NET-169' or 'DENSE-NET-201'\n",
        "    #---- nnIsTrained - if True, uses pre-trained version of the network (pre-trained on imagenet)\n",
        "    #---- nnClassCount - number of output classes\n",
        "    #---- trBatchSize - batch size\n",
        "    #---- trMaxEpoch - number of epochs\n",
        "    #---- transResize - size of the image to scale down to (not used in current implementation)\n",
        "    #---- transCrop - size of the cropped image\n",
        "    #---- launchTimestamp - date/time, used to assign unique name for the checkpoint file\n",
        "    #---- checkpoint - if not None loads the model and continues training\n",
        "\n",
        "    def test (pathDirData, pathFileTest, pathModel, nnArchitecture, nnClassCount, nnIsTrained, trBatchSize, transResize, transCrop, launchTimeStamp):\n",
        "\n",
        "\n",
        "        CLASS_NAMES = [ 'Atelectasis', 'Cardiomegaly', 'Effusion', 'Infiltration', 'Mass', 'Nodule', 'Pneumonia',\n",
        "                'Pneumothorax', 'Consolidation', 'Edema', 'Emphysema', 'Fibrosis', 'Pleural_Thickening', 'Hernia']\n",
        "\n",
        "        cudnn.benchmark = True\n",
        "\n",
        "        #-------------------- SETTINGS: NETWORK ARCHITECTURE, MODEL LOAD\n",
        "        if nnArchitecture == 'DENSE-NET-121': model = DenseNet121(nnClassCount, nnIsTrained).cuda()\n",
        "        elif nnArchitecture == 'DENSE-NET-169': model = DenseNet169(nnClassCount, nnIsTrained).cuda()\n",
        "        elif nnArchitecture == 'DENSE-NET-201': model = DenseNet201(nnClassCount, nnIsTrained).cuda()\n",
        "\n",
        "        model = torch.nn.DataParallel(model).cuda()\n",
        "\n",
        "        modelCheckpoint = torch.load(pathModel)\n",
        "        model.load_state_dict(modelCheckpoint['state_dict'])\n",
        "\n",
        "        #-------------------- SETTINGS: DATA TRANSFORMS, TEN CROPS\n",
        "        normalize = transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "\n",
        "        #-------------------- SETTINGS: DATASET BUILDERS\n",
        "        transformList = []\n",
        "        transformList.append(transforms.Resize(transResize))\n",
        "        transformList.append(transforms.TenCrop(transCrop))\n",
        "        transformList.append(transforms.Lambda(lambda crops: torch.stack([transforms.ToTensor()(crop) for crop in crops])))\n",
        "        transformList.append(transforms.Lambda(lambda crops: torch.stack([normalize(crop) for crop in crops])))\n",
        "        transformSequence=transforms.Compose(transformList)\n",
        "\n",
        "        datasetTest = DatasetGenerator(pathImageDirectory=pathDirData, pathDatasetFile=pathFileTest, transform=transformSequence)\n",
        "        dataLoaderTest = DataLoader(dataset=datasetTest, batch_size=trBatchSize, num_workers=8, shuffle=False, pin_memory=True)\n",
        "\n",
        "        outGT = torch.FloatTensor().cuda()\n",
        "        outPRED = torch.FloatTensor().cuda()\n",
        "\n",
        "        model.eval()\n",
        "\n",
        "        for i, (input, target) in enumerate(dataLoaderTest):\n",
        "\n",
        "            target = target.cuda()\n",
        "            outGT = torch.cat((outGT, target), 0)\n",
        "\n",
        "            bs, n_crops, c, h, w = input.size()\n",
        "\n",
        "            varInput = torch.autograd.Variable(input.view(-1, c, h, w).cuda(), volatile=True)\n",
        "\n",
        "            out = model(varInput)\n",
        "            outMean = out.view(bs, n_crops, -1).mean(1)\n",
        "\n",
        "            outPRED = torch.cat((outPRED, outMean.data), 0)\n",
        "\n",
        "        aurocIndividual = ChexnetTrainer.computeAUROC(outGT, outPRED, nnClassCount)\n",
        "        aurocMean = np.array(aurocIndividual).mean()\n",
        "\n",
        "        print ('AUROC mean ', aurocMean)\n",
        "\n",
        "        for i in range (0, len(aurocIndividual)):\n",
        "            print (CLASS_NAMES[i], ' ', aurocIndividual[i])\n",
        "        return"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}